{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11C5lyBEiNRRcN1mEiNZt1GuoDJKslKLZ",
      "authorship_tag": "ABX9TyPHBJpD6oaet5sTupSNIu1R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramesh8/ner-train-google-colab/blob/main/learningd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "id": "r5HNGPjRhs2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade spacy\n"
      ],
      "metadata": {
        "id": "tZEjT7OAh-gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U 'spacy[cuda-autodetect]'"
      ],
      "metadata": {
        "id": "z9Wksl3jY8pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0.tar.gz"
      ],
      "metadata": {
        "id": "OJ1yrmWnjP1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "gpu = spacy.prefer_gpu()\n",
        "print(gpu)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"This is Ramesh\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "dbk4MXEciUqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "rcL1sZLHYe7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "id": "LewBvzbEj7tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "iE_lSBXzkCLr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/drive\")"
      ],
      "metadata": {
        "id": "D_TpytuPohVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "\n",
        "json_data = []\n",
        "with zipfile.ZipFile(\"/drive/My Drive/ML/NLP/NER/entities.zip\", \"r\") as z:\n",
        "  for fname in z.namelist():\n",
        "    print(fname)\n",
        "    with z.open(fname) as f:\n",
        "      data = f.read()\n",
        "      json_data = json.loads(data.decode(\"utf-8\"))\n",
        "      print(f\"{len(json_data)} entities found\" )\n",
        "\n",
        "for i, group in enumerate(grouper(json_data, split_size)):\n",
        "    with open(f'splits\\entities_split_{i}.json', 'w') as opfile:\n",
        "        groupft = [gi for gi in list(group) if gi]        \n",
        "        json.dump(groupft, opfile)"
      ],
      "metadata": {
        "id": "QgCuf45Nkup2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from itertools import zip_longest\n",
        "\n",
        "split_size = 500\n",
        "\n",
        "def grouper(iterable, n, fillvalue=None):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(fillvalue=fillvalue, *args)"
      ],
      "metadata": {
        "id": "jsMVYhccZtB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "\n",
        "json_data = []\n",
        "with zipfile.ZipFile(\"/drive/My Drive/ML/NLP/NER/entities.zip\", \"r\") as z:\n",
        "  for fname in z.namelist():\n",
        "    print(fname)\n",
        "    with z.open(fname) as f:\n",
        "      data = f.read()\n",
        "      json_data = json.loads(data.decode(\"utf-8\"))\n",
        "      print(f\"{len(json_data)} entities found\" )\n",
        "\n",
        "for i, group in enumerate(grouper(json_data, split_size)):\n",
        "    with open(f'/drive/My Drive/ML/NLP/NER/splits/entities_split_{i}.json', 'w') as opfile:\n",
        "        groupft = [gi for gi in list(group) if gi]        \n",
        "        json.dump(groupft, opfile)"
      ],
      "metadata": {
        "id": "4eWaQxNqcyZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy convert \"/content/drive/MyDrive/ML/NLP/NER/splits/\" \"/content/drive/MyDrive/ML/NLP/NER/spacy/\" "
      ],
      "metadata": {
        "id": "q7stlL2fnDjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt"
      ],
      "metadata": {
        "id": "Y1c9wdROnwGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "splitpath = \"/drive/My Drive/ML/NLP/NER/splits/\"\n",
        "for file in os.listdir(splitpath):\n",
        "  jsonfile = os.path.join(splitpath, file)\n",
        "  targetfile = os.path.splitext(jsonfile)[0]+\".spacy\"\n",
        "  print(jsonfile, targetfile)"
      ],
      "metadata": {
        "id": "3QPdyDVOgKld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "import json\n",
        "import time\n",
        "import string\n",
        "\n",
        "class trainer:\n",
        "    def __init__(self,tdurl='data\\entities.json',spacy_file='train.spacy'):\n",
        "        self.file_url = tdurl\n",
        "        self.training_data_json = []\n",
        "        self.training_data = []\n",
        "        self.spacy_file_name = spacy_file\n",
        "        self.success_counter = 0\n",
        "        self.fail_counter = 0\n",
        "\n",
        "    def load_training_data(self):\n",
        "        try:\n",
        "            with open(self.file_url,mode='r',encoding='utf-8') as f:\n",
        "                self.training_data_json = json.load(f)\n",
        "                return True\n",
        "        except Exception as ex:\n",
        "            print(\"Failed to Load Trianing Data\")\n",
        "            return False\n",
        "\n",
        "    def prepare_training_data(self):\n",
        "        self.failed_list = []\n",
        "        if len(self.training_data_json)==0:\n",
        "            print(\"No Training Data\")\n",
        "            return False\n",
        "        self.training_data = []\n",
        "        noentscnt = 0\n",
        "        dupscnt = 0\n",
        "        for tdj in self.training_data_json:\n",
        "            _, pid, text,entities = tdj.values()\n",
        "            if len(entities)==0:\n",
        "                noentscnt += 1\n",
        "                continue\n",
        "            for i, ent in enumerate(entities):\n",
        "                if ent[0] == ent[1]:\n",
        "                    del entities[i]\n",
        "            if len(entities)==0:\n",
        "                noentscnt += 1\n",
        "                continue\n",
        "\n",
        "            ent = [tuple(en) for en in entities]\n",
        "            old_len = len(ent)\n",
        "            ent = list(set(ent))\n",
        "            new_len = len(ent)\n",
        "            self.training_data.append((text, {\"entities\":ent}))\n",
        "            if old_len != new_len:\n",
        "                dupscnt += (old_len-new_len)    \n",
        "\n",
        "    def convert_to_spacy_old(self):\n",
        "        self.fail_to_convert_list = []        \n",
        "        invalidChars = set(string.punctuation.replace(\"_\", \" \"))\n",
        "        nlp = spacy.load(\"en_core_web_lg\")\n",
        "        # nlp = spacy.blank(\"en\")\n",
        "        db = DocBin()    \n",
        "        skipped = 0\n",
        "        total = 0\n",
        "        for text, annotations in self.training_data:\n",
        "            doc = nlp.make_doc(text)\n",
        "            ents = []\n",
        "            \n",
        "            for start, end, label in annotations['entities']:\n",
        "                substr = text[start:end]\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "                total += 1\n",
        "                if span is None:\n",
        "                    skipped += 1\n",
        "                elif span.text.startswith(' ') or span.text.endswith(' '):\n",
        "                    skipped += 1\n",
        "                else:\n",
        "                    ents.append(span)            \n",
        "            _ents = list(set(ents))\n",
        "            try:    \n",
        "                doc.ents = _ents\n",
        "                db.add(doc)   \n",
        "                self.success_counter += 1             \n",
        "            except Exception as ex:\n",
        "                self.fail_counter += 1\n",
        "            \n",
        "        db.to_disk(self.spacy_file_name)\n",
        "        print(self.success_counter, self.fail_counter)\n"
      ],
      "metadata": {
        "id": "Blv1-F_l2EK7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "splitpath = \"/content/drive/MyDrive/ML/NLP/NER/splits/\"\n",
        "for file in os.listdir(splitpath):\n",
        "  jsonfile = os.path.join(splitpath, file)\n",
        "  targetfile = os.path.splitext(jsonfile)[0]+\".spacy\"\n",
        "  targetfile = targetfile.replace(\"splits\",\"spacy\")\n",
        "  tr = trainer(jsonfile,targetfile) \n",
        "  tr.load_training_data()\n",
        "  tr.prepare_training_data()\n",
        "  tr.convert_to_spacy_old()"
      ],
      "metadata": {
        "id": "STeiQsDu2IFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "RMgjil339qGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/ML/NLP/NER/"
      ],
      "metadata": {
        "id": "h1zLselb90ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "ImUZWi-h-u-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "ULua0dXnAE3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "mjAqA5NTA2O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train --paths.dev ./dev --gpu-id=0"
      ],
      "metadata": {
        "id": "uoxx3mjOBZUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train --paths.dev ./dev"
      ],
      "metadata": {
        "id": "8NHvs4DqDhQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}